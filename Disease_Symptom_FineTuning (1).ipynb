{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # Choose DiseaseAndSymptoms.csv and Disease precaution.csv\n"
      ],
      "metadata": {
        "id": "FoHGtKrQ0CPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "id": "UUhgguJH0CSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"DiseaseAndSymptoms.csv\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "znpwvnaH0CU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symptom_cols = [col for col in df.columns if col.startswith(\"Symptom_\")]\n",
        "print(symptom_cols)\n"
      ],
      "metadata": {
        "id": "IBPr2ZGs0CXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    # Join all symptoms into one text string\n",
        "    symptoms = []\n",
        "    for col in symptom_cols:\n",
        "        value = row[col]\n",
        "        if pd.notna(value):\n",
        "            symptoms.append(str(value).replace(\"_\", \" \").strip())\n",
        "\n",
        "    symptoms_text = \", \".join(symptoms)\n",
        "    disease = row[\"Disease\"]\n",
        "\n",
        "    entry = {\n",
        "        \"instruction\": \"Identify the disease pattern based on symptoms.\",\n",
        "        \"input\": symptoms_text,\n",
        "        \"output\": (\n",
        "            f\"Disease: {disease}\\n\"\n",
        "            f\"Explanation: These symptoms frequently match {disease} patterns in the dataset.\\n\"\n",
        "            \"Note: This is not medical or diagnostic advice. Consult a real doctor for health concerns.\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    data.append(entry)\n",
        "\n",
        "print(\"Example training entry:\")\n",
        "print(json.dumps(data[0], indent=2))\n",
        "print(\"Total entries created:\", len(data))\n"
      ],
      "metadata": {
        "id": "lQ72AD7h0CZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Shuffle the full dataset in-place\n",
        "random.shuffle(data)\n",
        "\n",
        "# 80% for training, 20% for testing\n",
        "split_idx = int(0.8 * len(data))\n",
        "train = data[:split_idx]\n",
        "test = data[split_idx:]\n",
        "\n",
        "print(\"Train size:\", len(train))\n",
        "print(\"Test size:\", len(test))\n"
      ],
      "metadata": {
        "id": "RZOU6xEG0Ccd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.jsonl\", \"w\") as f:\n",
        "    for item in train:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "with open(\"test.jsonl\", \"w\") as f:\n",
        "    for item in test:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(\"Saved files: train.jsonl and test.jsonl\")\n"
      ],
      "metadata": {
        "id": "ID0NmTLD0CfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "id": "C18H1gf90Ch7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft bitsandbytes accelerate\n"
      ],
      "metadata": {
        "id": "ZzCM1ft_0Cki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_ds = load_dataset(\"json\", data_files=\"train.jsonl\")[\"train\"]\n",
        "test_ds = load_dataset(\"json\", data_files=\"test.jsonl\")[\"train\"]\n",
        "\n",
        "print(train_ds[0])\n",
        "print(\"Train size:\", len(train_ds))\n",
        "print(\"Test size:\", len(test_ds))\n"
      ],
      "metadata": {
        "id": "LxyW7mdR0Cm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # public, no token required\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "eR11EjPX6fLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "RsyAfTh86fOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_ds = load_dataset(\"json\", data_files=\"train.jsonl\")[\"train\"]\n",
        "test_ds = load_dataset(\"json\", data_files=\"test.jsonl\")[\"train\"]\n",
        "\n",
        "print(\"Train size:\", len(train_ds))\n",
        "print(\"Test size:\", len(test_ds))\n",
        "print(train_ds[0])\n"
      ],
      "metadata": {
        "id": "t8RyWJBt6fQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_example(example):\n",
        "    text = (\n",
        "        f\"Instruction: {example['instruction']}\\n\"\n",
        "        f\"Input: {example['input']}\\n\"\n",
        "        f\"Output: {example['output']}\"\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "formatted_train = train_ds.map(format_example)\n",
        "formatted_test = test_ds.map(format_example)\n",
        "\n",
        "print(formatted_train[0][\"text\"])\n"
      ],
      "metadata": {
        "id": "YOqshAIl6fTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_train = formatted_train.map(tokenize_function, batched=True)\n",
        "tokenized_test = formatted_test.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the original text + fields we don't need for training\n",
        "tokenized_train = tokenized_train.remove_columns([\"instruction\", \"input\", \"output\", \"text\"])\n",
        "tokenized_test = tokenized_test.remove_columns([\"instruction\", \"input\", \"output\", \"text\"])\n",
        "\n",
        "import torch\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_test.set_format(\"torch\")\n",
        "\n",
        "print(tokenized_train[0][\"input_ids\"][:20])\n"
      ],
      "metadata": {
        "id": "MA08iSLn6fWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch   # <-- THIS FIXES YOUR ERROR\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./disease-llm-qlora\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_steps=20,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "def data_collator(features):\n",
        "    batch = {\n",
        "        \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
        "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
        "    }\n",
        "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "    return batch\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "nbbaTGWKHj0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"disease_llm_adapter\"\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(\"Saved fine-tuned adapter to:\", save_path)\n"
      ],
      "metadata": {
        "id": "taDWrG3bHj5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_disease_response(symptom_text, max_new_tokens=80):\n",
        "    instruction = \"Identify the disease pattern based on symptoms.\"\n",
        "    prompt = (\n",
        "        f\"Instruction: {instruction}\\n\"\n",
        "        f\"Input: {symptom_text}\\n\"\n",
        "        f\"Output:\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return full_text\n"
      ],
      "metadata": {
        "id": "dxLf8lSVHkAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model.eval()  # set model to evaluation mode\n",
        "\n",
        "# Use a small subset of the original test dataset (text form)\n",
        "small_eval = test_ds.select(range(30))   # 30 examples is enough for a confusion matrix\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for i, example in enumerate(small_eval):\n",
        "    # ----- True disease from ground-truth -----\n",
        "    gt_line = example[\"output\"].split(\"\\n\")[0]   # first line: \"Disease: XYZ\"\n",
        "    gt_disease = gt_line.replace(\"Disease:\", \"\").strip()\n",
        "    true_labels.append(gt_disease)\n",
        "\n",
        "    # ----- Model prediction -----\n",
        "    gen_text = generate_disease_response(example[\"input\"], max_new_tokens=40)\n",
        "\n",
        "    pred_disease = \"Unknown\"\n",
        "    if \"Disease:\" in gen_text:\n",
        "        try:\n",
        "            pred_disease = gen_text.split(\"Disease:\")[1].split(\"\\n\")[0].strip()\n",
        "        except:\n",
        "            pred_disease = \"Unknown\"\n",
        "\n",
        "    pred_labels.append(pred_disease)\n",
        "\n",
        "    # Progress print so you know it's moving\n",
        "    if (i + 1) % 5 == 0 or (i + 1) == len(small_eval):\n",
        "        print(f\"Processed {i+1}/{len(small_eval)} examples\")\n",
        "\n",
        "print(\"Done. Collected labels for:\", len(true_labels), \"examples.\")\n"
      ],
      "metadata": {
        "id": "HNqh0YkWHkFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "classes = sorted(list(set(true_labels)))   # unique actual diseases\n",
        "\n",
        "cm = confusion_matrix(true_labels, pred_labels, labels=classes)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "im = plt.imshow(cm, interpolation=\"nearest\")\n",
        "plt.title(\"Disease Prediction Confusion Matrix\")\n",
        "plt.colorbar(im)\n",
        "\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=90)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "plt.xlabel(\"Predicted Disease\")\n",
        "plt.ylabel(\"Actual Disease\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix.png\", dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved confusion_matrix.png\")\n"
      ],
      "metadata": {
        "id": "Jc365ZVDHkIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_symptoms = \"Fever, headache, body pain\"\n",
        "\n",
        "demo_output = generate_disease_response(demo_symptoms, max_new_tokens=80)\n",
        "print(demo_output)\n"
      ],
      "metadata": {
        "id": "vTVzaGSdHkLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample_output_demo_query.txt\", \"w\") as f:\n",
        "    f.write(\"Symptoms: Fever, headache, body pain\\n\\n\")\n",
        "    f.write(\"Model response:\\n\")\n",
        "    f.write(demo_output)\n",
        "\n",
        "print(\"Saved: sample_output_demo_query.txt\")\n"
      ],
      "metadata": {
        "id": "-TsTeq2GHkO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    \"fever, chills, cough, sore throat\",                    # maybe flu / viral\n",
        "    \"abdominal pain, diarrhea, vomiting, loss of appetite\", # gastro/intestinal\n",
        "    \"joint pain, swelling, stiffness, fatigue\",             # arthritis/rheumatic\n",
        "]\n",
        "\n",
        "for i, symptoms in enumerate(test_cases, start=1):\n",
        "    print(f\"\\n=== Test case {i} ===\")\n",
        "    print(\"Symptoms:\", symptoms)\n",
        "    response = generate_disease_response(symptoms, max_new_tokens=80)\n",
        "    print(\"Model response:\\n\", response)\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "fafjYYiHHkSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample_output_demo_query.txt\", \"w\") as f:\n",
        "    f.write(\"Symptoms: Fever, headache, body pain\\n\\n\")\n",
        "    f.write(\"Model response:\\n\")\n",
        "    f.write(demo_output)\n",
        "\n",
        "print(\"Saved: sample_output_demo_query.txt\")\n"
      ],
      "metadata": {
        "id": "Aehm4uApXlJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "id": "6G9ES1Kz6fho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r disease_llm_adapter.zip disease_llm_adapter\n"
      ],
      "metadata": {
        "id": "e5YNCYl6Zs8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "id": "k9uotJmzZs_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"train.jsonl\")\n",
        "files.download(\"test.jsonl\")\n",
        "files.download(\"confusion_matrix.png\")\n",
        "files.download(\"sample_output_demo_query.txt\")\n",
        "files.download(\"disease_llm_adapter.zip\")\n"
      ],
      "metadata": {
        "id": "V-hEOWkQZtCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Edit â†’ Clear all outputs\n"
      ],
      "metadata": {
        "id": "GGmNnkZUZtEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Clear all outputs\n"
      ],
      "metadata": {
        "id": "LWmvmX5SZtGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}